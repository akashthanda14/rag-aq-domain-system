"""
Response Generation Module

This module handles the generation of responses using retrieved context and LLMs.
It implements prompt engineering strategies to produce accurate, contextual answers.

Key Responsibilities:
1. Construct prompts with retrieved context
2. Interface with various LLM providers (OpenAI, HuggingFace, etc.)
3. Generate responses with source attribution
4. Handle streaming responses
5. Implement safety and quality checks

Design Principles:
- Flexible LLM backend support
- Clear source attribution
- Prompt engineering best practices
- Error handling and fallbacks
"""

import os
from typing import List, Dict, Optional, Union
from pathlib import Path


class LLMInterface:
    """
    Base class for LLM interfaces.
    Allows easy switching between different LLM providers.
    """
    
    def __init__(self, model_name: str, temperature: float = 0.7):
        """
        Initialize LLM interface.
        
        Args:
            model_name: Name of the model to use
            temperature: Sampling temperature (0.0 to 1.0)
        """
        self.model_name = model_name
        self.temperature = temperature
    
    def generate(self, prompt: str, max_tokens: int = 500) -> str:
        """
        Generate response from prompt.
        
        Args:
            prompt: Input prompt
            max_tokens: Maximum tokens to generate
            
        Returns:
            Generated text
        """
        raise NotImplementedError("Subclasses must implement generate method")


class OpenAILLM(LLMInterface):
    """
    Interface for OpenAI's GPT models.
    """
    
    def __init__(
        self,
        model_name: str = "gpt-3.5-turbo",
        api_key: Optional[str] = None,
        temperature: float = 0.7
    ):
        """
        Initialize OpenAI LLM.
        
        Args:
            model_name: OpenAI model name (e.g., gpt-4, gpt-3.5-turbo)
            api_key: OpenAI API key (or set OPENAI_API_KEY environment variable)
            temperature: Sampling temperature
        """
        super().__init__(model_name, temperature)
        # TODO: Initialize OpenAI client
        # import openai
        # self.client = openai.OpenAI(api_key=api_key)
        
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        print(f"[Placeholder] Initialized OpenAI LLM: {model_name}")
    
    def generate(self, prompt: str, max_tokens: int = 500) -> str:
        """
        Generate response using OpenAI API.
        
        Args:
            prompt: Input prompt
            max_tokens: Maximum tokens to generate
            
        Returns:
            Generated text
        """
        # TODO: Implement OpenAI API call
        # response = self.client.chat.completions.create(
        #     model=self.model_name,
        #     messages=[
        #         {"role": "system", "content": "You are a helpful assistant."},
        #         {"role": "user", "content": prompt}
        #     ],
        #     temperature=self.temperature,
        #     max_tokens=max_tokens
        # )
        # return response.choices[0].message.content
        
        # Placeholder response
        return f"[Placeholder Response] This would be generated by {self.model_name} based on the prompt."


class HuggingFaceLLM(LLMInterface):
    """
    Interface for HuggingFace transformers models.
    """
    
    def __init__(
        self,
        model_name: str = "google/flan-t5-base",
        temperature: float = 0.7,
        device: str = "cpu"
    ):
        """
        Initialize HuggingFace LLM.
        
        Args:
            model_name: HuggingFace model name
            temperature: Sampling temperature
            device: Device to run on ("cpu", "cuda")
        """
        super().__init__(model_name, temperature)
        self.device = device
        # TODO: Initialize model and tokenizer
        # from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
        # self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        # self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        # self.model.to(device)
        
        self.model = None
        self.tokenizer = None
        print(f"[Placeholder] Initialized HuggingFace LLM: {model_name}")
    
    def generate(self, prompt: str, max_tokens: int = 500) -> str:
        """
        Generate response using HuggingFace model.
        
        Args:
            prompt: Input prompt
            max_tokens: Maximum tokens to generate
            
        Returns:
            Generated text
        """
        # TODO: Implement HuggingFace generation
        # inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        # outputs = self.model.generate(
        #     **inputs,
        #     max_length=max_tokens,
        #     temperature=self.temperature,
        #     do_sample=True
        # )
        # return self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Placeholder response
        return f"[Placeholder Response] This would be generated by {self.model_name}."


class PromptTemplate:
    """
    Template for constructing prompts with retrieved context.
    """
    
    # Default RAG prompt template
    DEFAULT_TEMPLATE = """You are a helpful AI assistant. Answer the user's question based on the provided context.

Context:
{context}

Question: {question}

Instructions:
- Provide a clear and accurate answer based on the context
- If the context doesn't contain enough information, say so
- Cite the relevant parts of the context in your answer
- Be concise but thorough

Answer:"""
    
    # Template with source attribution
    ATTRIBUTED_TEMPLATE = """You are a helpful AI assistant. Answer the user's question using the provided context sources.

{context_with_sources}

Question: {question}

Instructions:
- Base your answer strictly on the provided context
- Reference specific sources using [Source X] notation
- If the context is insufficient, clearly state what information is missing
- Maintain a professional and academic tone

Answer:"""
    
    def __init__(self, template: Optional[str] = None):
        """
        Initialize prompt template.
        
        Args:
            template: Custom template string (uses default if None)
        """
        self.template = template or self.DEFAULT_TEMPLATE
    
    def format(self, question: str, context: Union[str, List[str]], **kwargs) -> str:
        """
        Format the prompt with question and context.
        
        Args:
            question: User's question
            context: Retrieved context (string or list of strings)
            **kwargs: Additional variables for the template
            
        Returns:
            Formatted prompt string
        """
        # Convert context list to string if needed
        if isinstance(context, list):
            context_str = "\n\n".join(context)
        else:
            context_str = context
        
        # Format template
        prompt = self.template.format(
            question=question,
            context=context_str,
            **kwargs
        )
        
        return prompt
    
    def format_with_sources(self, question: str, retrieved_docs: List[Dict]) -> str:
        """
        Format prompt with numbered source attribution.
        
        Args:
            question: User's question
            retrieved_docs: List of retrieved document dictionaries
            
        Returns:
            Formatted prompt with source citations
        """
        # Build context with source numbers
        context_parts = []
        for i, doc in enumerate(retrieved_docs, 1):
            text = doc.get('text', str(doc.get('document', '')))
            source_info = doc.get('metadata', {}).get('source', 'Unknown')
            context_parts.append(f"[Source {i}] ({source_info}):\n{text}")
        
        context_with_sources = "\n\n".join(context_parts)
        
        # Use attributed template
        template = PromptTemplate(self.ATTRIBUTED_TEMPLATE)
        return template.format(
            question=question,
            context_with_sources=context_with_sources
        )


class ResponseGenerator:
    """
    Main class for generating responses using RAG.
    Combines retrieval results with LLM generation.
    """
    
    def __init__(
        self,
        llm_type: str = "openai",
        model_name: Optional[str] = None,
        temperature: float = 0.7,
        prompt_template: Optional[str] = None
    ):
        """
        Initialize ResponseGenerator.
        
        Args:
            llm_type: Type of LLM ("openai" or "huggingface")
            model_name: Specific model name (uses defaults if None)
            temperature: Sampling temperature for generation
            prompt_template: Custom prompt template
        """
        self.llm_type = llm_type
        self.temperature = temperature
        
        # Initialize LLM
        if llm_type == "openai":
            model_name = model_name or "gpt-3.5-turbo"
            self.llm = OpenAILLM(model_name, temperature=temperature)
        elif llm_type == "huggingface":
            model_name = model_name or "google/flan-t5-base"
            self.llm = HuggingFaceLLM(model_name, temperature=temperature)
        else:
            raise ValueError(f"Unsupported LLM type: {llm_type}")
        
        # Initialize prompt template
        self.prompt_template = PromptTemplate(prompt_template)
    
    def generate(
        self,
        question: str,
        retrieved_docs: List[Dict],
        max_tokens: int = 500,
        include_sources: bool = True
    ) -> Dict[str, any]:
        """
        Generate response for a question using retrieved documents.
        
        Args:
            question: User's question
            retrieved_docs: List of retrieved document dictionaries
            max_tokens: Maximum tokens to generate
            include_sources: Whether to include source attribution
            
        Returns:
            Dictionary with answer and metadata
        """
        # TODO: Implement response generation pipeline
        # Steps:
        # 1. Extract context from retrieved documents
        # 2. Build prompt with template
        # 3. Generate response using LLM
        # 4. Post-process and add metadata
        # 5. Return structured response
        
        if not retrieved_docs:
            return {
                'answer': "I don't have enough information to answer this question.",
                'sources': [],
                'confidence': 'low'
            }
        
        # Build prompt
        if include_sources:
            prompt = self.prompt_template.format_with_sources(question, retrieved_docs)
        else:
            context = [doc.get('text', str(doc.get('document', ''))) for doc in retrieved_docs]
            prompt = self.prompt_template.format(question, context)
        
        # Generate response
        try:
            answer = self.llm.generate(prompt, max_tokens=max_tokens)
        except Exception as e:
            print(f"Error generating response: {e}")
            answer = "I encountered an error while generating the response. Please try again."
        
        # Extract sources
        sources = []
        for doc in retrieved_docs:
            source_info = {
                'text': doc.get('text', '')[:200] + '...',  # Truncate for display
                'metadata': doc.get('metadata', {})
            }
            if 'score' in doc:
                source_info['relevance_score'] = doc['score']
            sources.append(source_info)
        
        # Build response
        response = {
            'answer': answer,
            'sources': sources,
            'num_sources': len(sources),
            'question': question
        }
        
        # Add confidence estimate (placeholder)
        if len(retrieved_docs) >= 3 and all(doc.get('score', 0) > 0.7 for doc in retrieved_docs[:3]):
            response['confidence'] = 'high'
        elif len(retrieved_docs) >= 2 and any(doc.get('score', 0) > 0.6 for doc in retrieved_docs[:2]):
            response['confidence'] = 'medium'
        else:
            response['confidence'] = 'low'
        
        return response
    
    def generate_streaming(
        self,
        question: str,
        retrieved_docs: List[Dict],
        max_tokens: int = 500
    ):
        """
        Generate response with streaming (yields chunks as they're generated).
        
        Args:
            question: User's question
            retrieved_docs: List of retrieved documents
            max_tokens: Maximum tokens to generate
            
        Yields:
            Response chunks as they're generated
        """
        # TODO: Implement streaming generation
        # This is useful for real-time UI updates
        # Requires streaming API support from LLM
        
        # Placeholder: yield full response at once
        response = self.generate(question, retrieved_docs, max_tokens, include_sources=False)
        yield response['answer']
    
    def format_response_for_display(self, response: Dict) -> str:
        """
        Format response dictionary as human-readable text.
        
        Args:
            response: Response dictionary from generate()
            
        Returns:
            Formatted string for display
        """
        output = []
        output.append("=" * 80)
        output.append(f"Question: {response['question']}")
        output.append("=" * 80)
        output.append(f"\nAnswer:\n{response['answer']}\n")
        
        if response.get('sources'):
            output.append(f"\nSources ({response['num_sources']}):")
            for i, source in enumerate(response['sources'], 1):
                output.append(f"\n[{i}] {source['text']}")
                if 'source' in source['metadata']:
                    output.append(f"    From: {source['metadata']['source']}")
                if 'relevance_score' in source:
                    output.append(f"    Relevance: {source['relevance_score']:.3f}")
        
        output.append(f"\nConfidence: {response.get('confidence', 'unknown')}")
        output.append("=" * 80)
        
        return "\n".join(output)


# Example usage and testing
if __name__ == "__main__":
    """
    Example usage of the ResponseGenerator module.
    This demonstrates how to generate responses using RAG.
    """
    
    # Initialize generator
    generator = ResponseGenerator(
        llm_type="openai",
        model_name="gpt-3.5-turbo",
        temperature=0.7
    )
    
    # Sample retrieved documents
    retrieved_docs = [
        {
            'text': 'RAG (Retrieval-Augmented Generation) combines information retrieval with text generation.',
            'score': 0.85,
            'metadata': {'source': 'rag_introduction.pdf', 'page': 1}
        },
        {
            'text': 'The retrieval component finds relevant documents from a knowledge base.',
            'score': 0.78,
            'metadata': {'source': 'rag_architecture.pdf', 'page': 3}
        },
        {
            'text': 'The generation component uses an LLM to produce answers based on retrieved context.',
            'score': 0.72,
            'metadata': {'source': 'llm_guide.pdf', 'page': 15}
        }
    ]
    
    # Generate response
    question = "How does RAG work?"
    response = generator.generate(question, retrieved_docs, include_sources=True)
    
    # Display formatted response
    print(generator.format_response_for_display(response))
    
    # Example with custom prompt template
    custom_template = """Based on the following information:

{context}

Please answer: {question}

Provide a brief, focused answer."""
    
    custom_generator = ResponseGenerator(
        llm_type="openai",
        prompt_template=custom_template
    )
    
    print("\n\n--- Custom Template Example ---")
    custom_response = custom_generator.generate(
        question,
        retrieved_docs,
        include_sources=False
    )
    print(custom_response['answer'])
